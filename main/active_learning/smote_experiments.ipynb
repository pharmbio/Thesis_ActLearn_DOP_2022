{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pandas import read_excel\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import random\n",
    "import timeit\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, classification_report, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn import svm, tree    #https://scikit-learn.org/stable/modules/svm.html\n",
    "                                 #https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "from modAL.models import ActiveLearner             #https://modal-python.readthedocs.io/en/latest/content/models/ActiveLearner.html\n",
    "from modAL.uncertainty import entropy_sampling     #https://modal-python.readthedocs.io/en/latest/content/apireference/uncertainty.html\n",
    "from modAL.disagreement import vote_entropy_sampling\n",
    "\n",
    "from imblearn.over_sampling import SMOTEN\n",
    "from imblearn.under_sampling import EditedNearestNeighbours, ClusterCentroids\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/jovyan/Thesis_ActLearn_DOP_2022/main/active_learning/')\n",
    "import functions as fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data sets\n",
    "full_data_BatchA = pd.read_csv('/home/jovyan/Thesis_ActLearn_DOP_2022/main/active_learning/data/full_data_BatchA.csv')\n",
    "y = full_data_BatchA['Label'].to_numpy()\n",
    "X_morgan = full_data_BatchA.drop(['Label'], axis = 1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into 80:20 ration\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_morgan, y, test_size = 0.2, random_state = 0)\n",
    "  \n",
    "# describes info about train and test set\n",
    "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
    "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
    "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
    "print(\"Number transactions y_test dataset: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression object\n",
    "lr = LogisticRegression()\n",
    "# train the model on train set\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_test)\n",
    "\n",
    "# print classification report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n",
    "\n",
    "# over-sampling minority class : 0.1 corresponds to 100 of oversampling\n",
    "sampler = SMOTEN(sampling_strategy = .1 n_jobs= -1, random_state=0)\n",
    "X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1 = LogisticRegression()\n",
    "lr1.fit(X_train_res, y_train_res.ravel())\n",
    "predictions1 = lr1.predict(X_test)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(y_test, predictions1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(sum(y_train == 0)*(50/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n",
    "\n",
    "# \n",
    "kmeans = KMeans(n_clusters=int(sum(y_train == 0)*(50/100)), random_state=0)\n",
    "cc = ClusterCentroids(random_state=42, estimator = kmeans)\n",
    "X_train_res, y_train_res = cc.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LogisticRegression()\n",
    "lr2.fit(X_train_res, y_train_res.ravel())\n",
    "predictions2 = lr2.predict(X_test)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(y_test, predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a basic experiment to select the rates of undersampling and oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firts I want to split the positive points in a uniforme way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 51 counts of label '1' in the whole dataset\n",
      "We have 5120 counts of label '0' in the whole dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"We have {} counts of label '1' in the whole dataset\".format(sum(y == 1)))\n",
    "print(\"We have {} counts of label '0' in the whole dataset\".format(sum(y == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we have 41 counts of label '1' in the train dataset\n",
      "Now we have 10 counts of label '1' in the test dataset\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_morgan, y, test_size=0.2, stratify=y, random_state=6752)\n",
    "print(\"Now we have {} counts of label '1' in the train dataset\".format(sum(y_train == 1)))\n",
    "print(\"Now we have {} counts of label '1' in the test dataset\".format(sum(y_test == 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 4095, 1: 41})\n",
      "Counter({0: 4095, 1: 409})\n",
      "Counter({0: 818, 1: 409})\n"
     ]
    }
   ],
   "source": [
    "# example of combining random oversampling and undersampling for imbalanced data\n",
    "from collections import Counter\n",
    "print(Counter(y_train))\n",
    "# define oversampling strategy\n",
    "over = SMOTEN(sampling_strategy = 0.1, n_jobs= -1, random_state=0)\n",
    "# fit and apply the transform\n",
    "X, y = over.fit_resample(X_train, y_train)\n",
    "# summarize class distribution\n",
    "print(Counter(y))\n",
    "# define undersampling strategy\n",
    "under = ClusterCentroids(sampling_strategy = 0.5, random_state = 42)\n",
    "# fit and apply the transform\n",
    "X, y = under.fit_resample(X, y)\n",
    "# summarize class distribution\n",
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.97      1025\n",
      "           1       0.03      0.20      0.06        10\n",
      "\n",
      "    accuracy                           0.94      1035\n",
      "   macro avg       0.51      0.57      0.51      1035\n",
      "weighted avg       0.98      0.94      0.96      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr2 = LogisticRegression()\n",
    "lr2.fit(X, y.ravel())\n",
    "predictions2 = lr2.predict(X_test)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(y_test, predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05714285714285715\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_test,predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
